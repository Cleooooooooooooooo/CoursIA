{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook : Génération d'Images, Low-Code AI, Function Calling, RAG\n",
    "\n",
    "Dans ce notebook, nous allons tour à tour découvrir :\n",
    "\n",
    "1. Comment générer des **images** à partir de prompts en texte (ex: DALL-E, Midjourney).\n",
    "2. Comment créer des **applications low-code** enrichies par l'IA, grâce à **Power Platform** (Copilot, AI Builder).\n",
    "3. **Function Calling** côté OpenAI : structurer les réponses d’un LLM pour déclencher des actions.\n",
    "4. **RAG** (Retrieval Augmented Generation) et **bases vectorielles** (indexation, recherche sémantique, chunking, etc.).\n",
    "\n",
    "\n",
    "\n",
    "## Prérequis & Installation\n",
    "\n",
    "- **Python 3.9+** (ou version ultérieure).\n",
    "- Un compte [OpenAI](https://platform.openai.com/) et une clé d’API valide.\n",
    "- Le fichier `.env` contenant votre clé d’API :\n",
    "OPENAI_API_KEY=sk-...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Pourquoi générer des images ?\n",
    "\n",
    "La génération d'images via IA offre une multitude d'applications : \n",
    "- prototypes visuels (design, marketing, art),\n",
    "- éducation (illustrations d’un cours, images pour un devoir),\n",
    "- game dev (concept art), \n",
    "- usage créatif (avatars, logos).\n",
    "\n",
    "Deux modèles phares : \n",
    "- **DALL-E** (OpenAI)\n",
    "- **Midjourney**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger le fichier .env pour la clé OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "\n",
    "# Config globale\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "try:\n",
    "    response = openai.images.generate(\n",
    "        prompt=\"Lapin sur un cheval tenant une sucette, dans un champ brumeux\"\n",
    "    )\n",
    "    # response est un ImagesResponse\n",
    "    image_url = response.data[0].url\n",
    "    print(\"Image URL:\", image_url)\n",
    "\n",
    "    # Téléchargement de l'image\n",
    "    img_data = requests.get(image_url).content\n",
    "    with open(\"my_image.png\",\"wb\") as f:\n",
    "        f.write(img_data)\n",
    "\n",
    "    # Ouverture\n",
    "    img = Image.open(\"my_image.png\")\n",
    "    img.show()\n",
    "\n",
    "except openai.APIConnectionError as e:\n",
    "    print(\"Erreur de connexion réseau:\", e)\n",
    "except openai.RateLimitError as e:\n",
    "    print(\"Limite atteinte ou quota dépassé:\", e)\n",
    "except openai.APIStatusError as e:\n",
    "    print(\"Erreur HTTP renvoyée par l'API (4xx, 5xx, etc.):\", e)\n",
    "except openai.APIError as e:\n",
    "    print(\"Autre erreur OpenAI:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Méta-prompts et usage responsable\n",
    "\n",
    "Pour gérer un usage plus responsable et filtrer des images non souhaitées, on peut ajouter un \n",
    "**meta-prompt** en amont, décrivant les restrictions (ex: Safe for Work, No adult content, etc.).\n",
    "\n",
    "Ex:\n",
    "You are an assistant that only generates children-friendly images. [... consignes ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Low-Code AI Apps (Power Platform)\n",
    "\n",
    "## 2.1 Introduction\n",
    "Power Platform inclut :\n",
    "- Power Apps (construction rapide d'apps)\n",
    "- Power Automate (workflows et automatisations)\n",
    "- Dataverse (stockage de données)\n",
    "- AI Builder (modèles IA pré-construits)\n",
    "- Copilot (assistant pour générer tables, flux, e-mails)\n",
    "\n",
    "Avantages : construction **no-code / low-code** pour mettre en place des solutions rapidement, \n",
    "y compris connectées à des services IA.\n",
    "\n",
    "\n",
    "## 2.2 Copilot dans Power Apps : Student Assignment Tracker\n",
    "\n",
    "Exemple : On veut un **Student Assignment Tracker**.\n",
    "\n",
    "1. Sur la home de [Power Apps](https://make.powerapps.com), on saisit dans la zone Copilot : \n",
    "   \"I want an app to track and manage student assignments.\"\n",
    "2. Copilot propose une table Dataverse (champs Title, DateDue, StudentName, etc.)\n",
    "3. Personnaliser la table (ajouter `StudentEmail`, etc.)\n",
    "4. Cliquer \"Create app\" => Copilot génère une **Canvas App** auto.\n",
    "5. Ajouter une page (screen) pour \"Envoyer un email\" (Prompt : \"I want a screen to send an email to the student\").\n",
    "\n",
    "On obtient en quelques clics un début d'application.\n",
    "\n",
    "\n",
    "## 2.3 Copilot dans Power Automate : Invoice Processing\n",
    "\n",
    "Même concept : Dans [Power Automate](https://make.powerautomate.com),\n",
    "on demande \"Process an invoice when it arrives in my mailbox\", \n",
    "Copilot propose un flux (trigger: new mail arrives + extractions + email)...\n",
    "\n",
    "On peut ensuite y intégrer **AI Builder** : \n",
    "- ex: le prébuilt model \"Invoice Processing\" pour extraire `supplier`, `amount`, etc.\n",
    "- stocker dans Dataverse, \n",
    "- email de confirmation.\n",
    "\n",
    "C’est un gros gain de temps pour la finance ou la logistique !\n",
    "\n",
    "\n",
    "# 3. Function Calling (OpenAI)\n",
    "\n",
    "\n",
    "## 3.1 Pourquoi ?\n",
    "\n",
    "Sans function calling, le LLM renvoie du texte non structuré. \n",
    "Difficile d’automatiser (ex: parse JSON, exécuter une fonction tierce).\n",
    "Avec function calling, on déclare un `schema` JSON, \n",
    "le LLM répond par un `function_call`: \n",
    "- Nom de la fonction \n",
    "- Arguments structurés\n",
    "\n",
    "Ensuite on exécute la fonction en Python (ou autre).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Find me a good course for a beginner developer to learn Azure.\"}\n",
    "]\n",
    "\n",
    "functions = [\n",
    "  {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"description\": \"Retrieves relevant courses based on role, product & level\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"role\":   {\"type\":\"string\",\"description\":\"the role of the user\"},\n",
    "        \"product\":{\"type\":\"string\",\"description\":\"the product/tech\"},\n",
    "        \"level\": {\"type\":\"string\",\"description\":\"the user skill level\"}\n",
    "      },\n",
    "      \"required\": [\"role\",\"product\",\"level\"]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "try:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  \n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\"  # laisse le LLM décider s’il appelle la fonction\n",
    "    )\n",
    "\n",
    "    print(\"Réponse brute:\\n\", response.choices[0].message)\n",
    "\n",
    "except openai.RateLimitError as e:\n",
    "    print(\"Limite atteinte:\", e)\n",
    "except openai.APIError as e:\n",
    "    print(\"Autre erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def search_courses(role,product,level):\n",
    "    # Ton code Python => Appel API Microsoft Learn\n",
    "    # On renvoie un JSON/string\n",
    "    return \"Liste de cours: Azure Fundamentals, etc.\"\n",
    "\n",
    "resp_msg = response.choices[0].message\n",
    "if resp_msg.function_call:\n",
    "    fn_name = resp_msg.function_call.name\n",
    "    fn_args = json.loads(resp_msg.function_call.arguments)\n",
    "    \n",
    "    # Exécuter la fonction Python correspondante\n",
    "    result = search_courses(**fn_args)\n",
    "\n",
    "    # On crée deux messages :\n",
    "    # 1) le function_call\n",
    "    # 2) le role=\"function\" + content du résultat\n",
    "    second_messages = [\n",
    "      {\"role\":\"assistant\",\"function_call\": {\"name\":fn_name,\"arguments\":resp_msg.function_call.arguments}},\n",
    "      {\"role\":\"function\",\"name\":fn_name,\"content\": result}\n",
    "    ]\n",
    "\n",
    "    # On relance le chat\n",
    "    final_resp = openai.chat.completions.create(\n",
    "       model=\"gpt-3.5-turbo\",\n",
    "       messages=messages + second_messages\n",
    "    )\n",
    "    print(\"Réponse finale:\\n\", final_resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retrieval Augmented Generation & Vector Databases\n",
    "\n",
    "## 4.1 Principe\n",
    "Un LLM (ex: GPT) a une limite : il ne connaît pas forcément nos documents internes. \n",
    "RAG => on stocke nos docs dans une base vectorielle (embeddings), \n",
    "puis à chaque question, on envoie au LLM les passages pertinents (retrieval + augmentation).\n",
    "\n",
    "## 4.2 Création d’une base vectorielle\n",
    "\n",
    "- On découpe (chunk) nos documents en petits segments (ex: 400 tokens).\n",
    "- On calcule embeddings (ex: text-embedding-ada-002).\n",
    "- On stocke : ex. Cosmos DB, Pinecone, ChromaDB, Elasticsearch, Qdrant, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn numpy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL du débat\n",
    "url = \"https://home.nps.gov/liho/learn/historyculture/debate1.htm\"\n",
    "\n",
    "# Requête HTTP\n",
    "response = requests.get(url)\n",
    "html = response.text  # contenu HTML sous forme de string\n",
    "\n",
    "# On parse avec BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Sélection du noeud principal.\n",
    "# Selon ton info: div.ColumnMain:nth-child(2)\n",
    "# (Le \"nth-child(2)\" est parfois incertain, on peut tenter un select plus large.)\n",
    "main_div = soup.select_one(\"div.ColumnMain\")\n",
    "\n",
    "if not main_div:\n",
    "    raise ValueError(\"Impossible de trouver le div.ColumnMain dans la page !\")\n",
    "\n",
    "# Extraction du texte brut (on sépare par \" \" pour éviter les collisions)\n",
    "debate_text = main_div.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Pour débogage:\n",
    "print(\"=== Longueur du texte récupéré:\", len(debate_text))\n",
    "print(debate_text[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"title\":\"First Debate: Ottawa, Illinois (NPS)\",\n",
    "            \"text\": debate_text\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    splitted = split_text_into_chunks(row[\"text\"], chunk_size=400, overlap=50)\n",
    "    for chunk in splitted:\n",
    "        rows.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(rows)\n",
    "print(\"Nombre de chunks =\", len(df_chunks))\n",
    "df_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialiser le client OpenAI (indispensable avec la nouvelle API)\n",
    "client = OpenAI()\n",
    "\n",
    "def create_embedding(text: str):\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=[text]  # ⚠️ Doit être une **liste**\n",
    "        )\n",
    "        return response.data[0].embedding  # Extraction correcte\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur lors de la génération d'embedding: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Générer les embeddings pour les chunks du DataFrame\n",
    "df_chunks[\"embedding\"] = df_chunks[\"chunk\"].apply(create_embedding)\n",
    "\n",
    "# Nettoyage des embeddings\n",
    "all_vectors = np.array([emb for emb in df_chunks[\"embedding\"] if emb is not None])  # Exclure None\n",
    "nn = NearestNeighbors(n_neighbors=3, metric=\"euclidean\")\n",
    "nn.fit(all_vectors)\n",
    "\n",
    "def retrieve(user_query: str) -> str:\n",
    "    try:\n",
    "        # Générer l'embedding de la requête\n",
    "        q_emb = create_embedding(user_query)\n",
    "        if q_emb is None:\n",
    "            return \"⚠️ Impossible de générer un embedding pour la requête.\"\n",
    "\n",
    "        dist, idx = nn.kneighbors([q_emb])\n",
    "\n",
    "        # Récupérer les meilleurs chunks\n",
    "        best_chunks = df_chunks.iloc[idx[0]][\"chunk\"].tolist()\n",
    "        prompt = user_query + \"\\n\\n\" + \"\\n\".join(best_chunks)\n",
    "\n",
    "        # ✅ Appel OpenAI corrigé\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content if response.choices else \"⚠️ Aucune réponse générée.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Erreur lors de la récupération : {str(e)}\"\n",
    "\n",
    "\n",
    "# 🔥 Test avec la question sur Lincoln\n",
    "question = \"What did Lincoln argue about slavery in that first debate?\"\n",
    "answer = retrieve(question)\n",
    "print(\"🔍 Réponse générée :\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion & Pistes\n",
    "\n",
    "Nous avons exploré :\n",
    "- la génération d’images (DALL-E, prompts, meta-prompts),\n",
    "- la création d’apps low-code Power Apps / Automate,\n",
    "- l’usage de Copilot & AI Builder pour des scénarios métiers (tracking, invoice),\n",
    "- la structuration des réponses via Function Calling,\n",
    "- RAG : indexer nos docs dans une base vectorielle et enrichir un LLM.\n",
    "\n",
    "Pistes d’exercices :\n",
    "- Améliorer les prompts d’images (température, variations, mask, etc.)\n",
    "- Créer un flux complet dans Power Automate avec AI Builder\n",
    "- Mettre en place Function Calling plus complexe (multi-fonctions, error-handling)\n",
    "- Stocker un doc plus large (ex: 10 pages PDF) en chunks + RAG\n",
    "\n",
    "Fin de la synthèse ! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
