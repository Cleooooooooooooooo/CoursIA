{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook : Génération d'Images, Low-Code AI, Function Calling, RAG\n",
    "\n",
    "Dans ce notebook, nous allons tour à tour découvrir :\n",
    "\n",
    "1. Comment générer des **images** à partir de prompts en texte (ex: DALL-E, Midjourney).\n",
    "2. Comment créer des **applications low-code** enrichies par l'IA, grâce à **Power Platform** (Copilot, AI Builder).\n",
    "3. **Function Calling** côté OpenAI : structurer les réponses d’un LLM pour déclencher des actions.\n",
    "4. **RAG** (Retrieval Augmented Generation) et **bases vectorielles** (indexation, recherche sémantique, chunking, etc.).\n",
    "\n",
    "\n",
    "\n",
    "## Prérequis & Installation\n",
    "\n",
    "- **Python 3.9+** (ou version ultérieure).\n",
    "- Un compte [OpenAI](https://platform.openai.com/) et une clé d’API valide.\n",
    "- Le fichier `.env` contenant votre clé d’API :\n",
    "OPENAI_API_KEY=sk-...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Pourquoi générer des images ?\n",
    "\n",
    "La génération d'images via IA offre une multitude d'applications : \n",
    "- prototypes visuels (design, marketing, art),\n",
    "- éducation (illustrations d’un cours, images pour un devoir),\n",
    "- game dev (concept art), \n",
    "- usage créatif (avatars, logos).\n",
    "\n",
    "Deux modèles phares : \n",
    "- **DALL-E** (OpenAI)\n",
    "- **Midjourney**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.57.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URL: https://oaidalleapiprodscus.blob.core.windows.net/private/org-3vPGVqYeKnTllNNI566kc9VD/user-f6xh2Ni7M3g4e6BRSpvSRb4A/img-wFVWcradlWiyGJlQRtEP889Y.png?st=2025-02-03T09%3A51%3A44Z&se=2025-02-03T11%3A51%3A44Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-02T16%3A41%3A05Z&ske=2025-02-03T16%3A41%3A05Z&sks=b&skv=2024-08-04&sig=uHSkp8FnrgFDFN2Mqw4D6kW6fFAkvgOV2oek4lToS/g%3D\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-3vPGVqYeKnTllNNI566kc9VD/user-f6xh2Ni7M3g4e6BRSpvSRb4A/img-wFVWcradlWiyGJlQRtEP889Y.png?st=2025-02-03T09%3A51%3A44Z&se=2025-02-03T11%3A51%3A44Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-02T16%3A41%3A05Z&ske=2025-02-03T16%3A41%3A05Z&sks=b&skv=2024-08-04&sig=uHSkp8FnrgFDFN2Mqw4D6kW6fFAkvgOV2oek4lToS/g%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "# from PIL import Image\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger le fichier .env pour la clé OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "\n",
    "# Config globale\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "try:\n",
    "    response = openai.images.generate(\n",
    "        prompt=\"Lapin sur un cheval tenant une sucette, dans un champ brumeux\"\n",
    "    )\n",
    "    # response est un ImagesResponse\n",
    "    image_url = response.data[0].url\n",
    "    print(\"Image URL:\", image_url)\n",
    "\n",
    "    # Téléchargement de l'image\n",
    "    # img_data = requests.get(image_url).content\n",
    "    # with open(\"my_image.png\",\"wb\") as f:\n",
    "    #     f.write(img_data)\n",
    "\n",
    "    # # Ouverture\n",
    "    # img = Image.open(\"my_image.png\")\n",
    "    # img.show()\n",
    "    \n",
    "    \n",
    "    image = Image(url= image_url)\n",
    "    display(image)\n",
    "\n",
    "except openai.APIConnectionError as e:\n",
    "    print(\"Erreur de connexion réseau:\", e)\n",
    "except openai.RateLimitError as e:\n",
    "    print(\"Limite atteinte ou quota dépassé:\", e)\n",
    "except openai.APIStatusError as e:\n",
    "    print(\"Erreur HTTP renvoyée par l'API (4xx, 5xx, etc.):\", e)\n",
    "except openai.APIError as e:\n",
    "    print(\"Autre erreur OpenAI:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Méta-prompts et usage responsable\n",
    "\n",
    "Pour gérer un usage plus responsable et filtrer des images non souhaitées, on peut ajouter un \n",
    "**meta-prompt** en amont, décrivant les restrictions (ex: Safe for Work, No adult content, etc.).\n",
    "\n",
    "Ex:\n",
    "You are an assistant that only generates children-friendly images. [... consignes ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Low-Code AI Apps (Power Platform)\n",
    "\n",
    "## 2.1 Introduction\n",
    "Power Platform inclut :\n",
    "- Power Apps (construction rapide d'apps)\n",
    "- Power Automate (workflows et automatisations)\n",
    "- Dataverse (stockage de données)\n",
    "- AI Builder (modèles IA pré-construits)\n",
    "- Copilot (assistant pour générer tables, flux, e-mails)\n",
    "\n",
    "Avantages : construction **no-code / low-code** pour mettre en place des solutions rapidement, \n",
    "y compris connectées à des services IA.\n",
    "\n",
    "\n",
    "## 2.2 Copilot dans Power Apps : Student Assignment Tracker\n",
    "\n",
    "Exemple : On veut un **Student Assignment Tracker**.\n",
    "\n",
    "1. Sur la home de [Power Apps](https://make.powerapps.com), on saisit dans la zone Copilot : \n",
    "   \"I want an app to track and manage student assignments.\"\n",
    "2. Copilot propose une table Dataverse (champs Title, DateDue, StudentName, etc.)\n",
    "3. Personnaliser la table (ajouter `StudentEmail`, etc.)\n",
    "4. Cliquer \"Create app\" => Copilot génère une **Canvas App** auto.\n",
    "5. Ajouter une page (screen) pour \"Envoyer un email\" (Prompt : \"I want a screen to send an email to the student\").\n",
    "\n",
    "On obtient en quelques clics un début d'application.\n",
    "\n",
    "\n",
    "## 2.3 Copilot dans Power Automate : Invoice Processing\n",
    "\n",
    "Même concept : Dans [Power Automate](https://make.powerautomate.com),\n",
    "on demande \"Process an invoice when it arrives in my mailbox\", \n",
    "Copilot propose un flux (trigger: new mail arrives + extractions + email)...\n",
    "\n",
    "On peut ensuite y intégrer **AI Builder** : \n",
    "- ex: le prébuilt model \"Invoice Processing\" pour extraire `supplier`, `amount`, etc.\n",
    "- stocker dans Dataverse, \n",
    "- email de confirmation.\n",
    "\n",
    "C’est un gros gain de temps pour la finance ou la logistique !\n",
    "\n",
    "\n",
    "# 3. Function Calling (OpenAI)\n",
    "\n",
    "\n",
    "## 3.1 Pourquoi ?\n",
    "\n",
    "Sans function calling, le LLM renvoie du texte non structuré. \n",
    "Difficile d’automatiser (ex: parse JSON, exécuter une fonction tierce).\n",
    "Avec function calling, on déclare un `schema` JSON, \n",
    "le LLM répond par un `function_call`: \n",
    "- Nom de la fonction \n",
    "- Arguments structurés\n",
    "\n",
    "Ensuite on exécute la fonction en Python (ou autre).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Find me a good course for a beginner developer to learn Azure.\"}\n",
    "]\n",
    "\n",
    "functions = [\n",
    "  {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"description\": \"Retrieves relevant courses based on role, product & level\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"role\":   {\"type\":\"string\",\"description\":\"the role of the user\"},\n",
    "        \"product\":{\"type\":\"string\",\"description\":\"the product/tech\"},\n",
    "        \"level\": {\"type\":\"string\",\"description\":\"the user skill level\"}\n",
    "      },\n",
    "      \"required\": [\"role\",\"product\",\"level\"]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "try:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  \n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\"  # laisse le LLM décider s’il appelle la fonction\n",
    "    )\n",
    "\n",
    "    print(\"Réponse brute:\\n\", response.choices[0].message)\n",
    "\n",
    "except openai.RateLimitError as e:\n",
    "    print(\"Limite atteinte:\", e)\n",
    "except openai.APIError as e:\n",
    "    print(\"Autre erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def search_courses(role,product,level):\n",
    "    # Ton code Python => Appel API Microsoft Learn\n",
    "    # On renvoie un JSON/string\n",
    "    return \"Liste de cours: Azure Fundamentals, etc.\"\n",
    "\n",
    "resp_msg = response.choices[0].message\n",
    "if resp_msg.function_call:\n",
    "    fn_name = resp_msg.function_call.name\n",
    "    fn_args = json.loads(resp_msg.function_call.arguments)\n",
    "    \n",
    "    # Exécuter la fonction Python correspondante\n",
    "    result = search_courses(**fn_args)\n",
    "\n",
    "    # On crée deux messages :\n",
    "    # 1) le function_call\n",
    "    # 2) le role=\"function\" + content du résultat\n",
    "    second_messages = [\n",
    "      {\"role\":\"assistant\",\"function_call\": {\"name\":fn_name,\"arguments\":resp_msg.function_call.arguments}},\n",
    "      {\"role\":\"function\",\"name\":fn_name,\"content\": result}\n",
    "    ]\n",
    "\n",
    "    # On relance le chat\n",
    "    final_resp = openai.chat.completions.create(\n",
    "       model=\"gpt-3.5-turbo\",\n",
    "       messages=messages + second_messages\n",
    "    )\n",
    "    print(\"Réponse finale:\\n\", final_resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retrieval Augmented Generation & Vector Databases\n",
    "\n",
    "## 4.1 Principe\n",
    "Un LLM (ex: GPT) a une limite : il ne connaît pas forcément nos documents internes. \n",
    "RAG => on stocke nos docs dans une base vectorielle (embeddings), \n",
    "puis à chaque question, on envoie au LLM les passages pertinents (retrieval + augmentation).\n",
    "\n",
    "## 4.2 Création d’une base vectorielle\n",
    "\n",
    "- On découpe (chunk) nos documents en petits segments (ex: 400 tokens).\n",
    "- On calcule embeddings (ex: text-embedding-ada-002).\n",
    "- On stocke : ex. Cosmos DB, Pinecone, ChromaDB, Elasticsearch, Qdrant, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn numpy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL du débat\n",
    "url = \"https://home.nps.gov/liho/learn/historyculture/debate1.htm\"\n",
    "\n",
    "# Requête HTTP\n",
    "response = requests.get(url)\n",
    "html = response.text  # contenu HTML sous forme de string\n",
    "\n",
    "# On parse avec BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Sélection du noeud principal.\n",
    "# Selon ton info: div.ColumnMain:nth-child(2)\n",
    "# (Le \"nth-child(2)\" est parfois incertain, on peut tenter un select plus large.)\n",
    "main_div = soup.select_one(\"div.ColumnMain\")\n",
    "\n",
    "if not main_div:\n",
    "    raise ValueError(\"Impossible de trouver le div.ColumnMain dans la page !\")\n",
    "\n",
    "# Extraction du texte brut (on sépare par \" \" pour éviter les collisions)\n",
    "debate_text = main_div.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Pour débogage:\n",
    "print(\"=== Longueur du texte récupéré:\", len(debate_text))\n",
    "print(debate_text[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"title\":\"First Debate: Ottawa, Illinois (NPS)\",\n",
    "            \"text\": debate_text\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    splitted = split_text_into_chunks(row[\"text\"], chunk_size=400, overlap=50)\n",
    "    for chunk in splitted:\n",
    "        rows.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(rows)\n",
    "print(\"Nombre de chunks =\", len(df_chunks))\n",
    "df_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialiser le client OpenAI (indispensable avec la nouvelle API)\n",
    "client = OpenAI()\n",
    "\n",
    "def create_embedding(text: str):\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=[text]  # ⚠️ Doit être une **liste**\n",
    "        )\n",
    "        return response.data[0].embedding  # Extraction correcte\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur lors de la génération d'embedding: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Générer les embeddings pour les chunks du DataFrame\n",
    "df_chunks[\"embedding\"] = df_chunks[\"chunk\"].apply(create_embedding)\n",
    "\n",
    "# Nettoyage des embeddings\n",
    "all_vectors = np.array([emb for emb in df_chunks[\"embedding\"] if emb is not None])  # Exclure None\n",
    "nn = NearestNeighbors(n_neighbors=3, metric=\"euclidean\")\n",
    "nn.fit(all_vectors)\n",
    "\n",
    "def retrieve(user_query: str) -> str:\n",
    "    try:\n",
    "        # Générer l'embedding de la requête\n",
    "        q_emb = create_embedding(user_query)\n",
    "        if q_emb is None:\n",
    "            return \"⚠️ Impossible de générer un embedding pour la requête.\"\n",
    "\n",
    "        dist, idx = nn.kneighbors([q_emb])\n",
    "\n",
    "        # Récupérer les meilleurs chunks\n",
    "        best_chunks = df_chunks.iloc[idx[0]][\"chunk\"].tolist()\n",
    "        prompt = user_query + \"\\n\\n\" + \"\\n\".join(best_chunks)\n",
    "\n",
    "        # ✅ Appel OpenAI corrigé\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content if response.choices else \"⚠️ Aucune réponse générée.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Erreur lors de la récupération : {str(e)}\"\n",
    "\n",
    "\n",
    "# 🔥 Test avec la question sur Lincoln\n",
    "question = \"What did Lincoln argue about slavery in that first debate?\"\n",
    "answer = retrieve(question)\n",
    "print(\"🔍 Réponse générée :\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion & Pistes\n",
    "\n",
    "Nous avons exploré :\n",
    "- la génération d’images (DALL-E, prompts, meta-prompts),\n",
    "- la création d’apps low-code Power Apps / Automate,\n",
    "- l’usage de Copilot & AI Builder pour des scénarios métiers (tracking, invoice),\n",
    "- la structuration des réponses via Function Calling,\n",
    "- RAG : indexer nos docs dans une base vectorielle et enrichir un LLM.\n",
    "\n",
    "Pistes d’exercices :\n",
    "- Améliorer les prompts d’images (température, variations, mask, etc.)\n",
    "- Créer un flux complet dans Power Automate avec AI Builder\n",
    "- Mettre en place Function Calling plus complexe (multi-fonctions, error-handling)\n",
    "- Stocker un doc plus large (ex: 10 pages PDF) en chunks + RAG\n",
    "\n",
    "Fin de la synthèse ! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
