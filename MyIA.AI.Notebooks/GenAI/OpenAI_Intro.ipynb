{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests de connections à l'API OpenAI d'Oobabooga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan global du Notebook\n",
    "\n",
    "1. **Introduction générale**  \n",
    "   - Contexte et définitions (IA Générative, LLMs, prompts, etc.)\n",
    "   - Brève présentation des enjeux éthiques et de la responsabilité\n",
    "\n",
    "2. **Premier exemple de code : Vérification d’environnement**  \n",
    "   - Installation/importation des bibliothèques (OpenAI ou Azure OpenAI)\n",
    "   - Test d’un prompt de base\n",
    "\n",
    "3. **Notions de base sur les prompts**  \n",
    "   - Tokens, embeddings, bases de la génération\n",
    "   - Exercices simples : tokenisation et génération\n",
    "\n",
    "4. **Exemple : Fabrications (Hallucinations) et fiabilité**  \n",
    "   - Démonstration d’un prompt volontairement ambigu\n",
    "   - Discussion sur la vérification des faits\n",
    "\n",
    "5. **Conclusion**  \n",
    "   - Rappel des grandes idées\n",
    "   - Activité suggérée (rédaction d’une synthèse ou extension)\n",
    "\n",
    "\n",
    "\n",
    "# Introduction à l'IA Générative\n",
    "\n",
    "Dans ce notebook, nous allons découvrir les bases de l'Intelligence Artificielle Générative. \n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre ce qu’est l’IA Générative et en quoi elle consiste (texte, images, audio…)\n",
    "- Découvrir le fonctionnement des modèles de langage de grande taille (LLMs)\n",
    "- Mettre en pratique de premières expérimentations simples avec les prompts\n",
    "- Aborder brièvement les questions de fiabilité (fabrications / “hallucinations”) et d’éthique\n",
    "\n",
    "## Qu’est-ce que l’IA Générative ?\n",
    "\n",
    "L’IA générative est une branche de l’apprentissage automatique qui **génère** de nouveaux contenus (texte, image, audio, code...) en se basant sur des modèles probabilistes entraînés sur de vastes quantités de données. Les modèles les plus avancés, souvent appelés **Large Language Models** (LLMs), utilisent des réseaux de neurones du type **Transformers** pour prédire et générer des séquences.\n",
    "\n",
    "Exemples concrets :\n",
    "- **ChatGPT** (OpenAI) : génération et compréhension de texte\n",
    "- **Stable Diffusion** : génération d’images\n",
    "- **Audiocraft**, **Whisper** : génération ou transcription audio\n",
    "- **Copilot** (GitHub) : génération de code, complétion et explications\n",
    "\n",
    "## Enjeux et limites\n",
    "- **Hallucinations / Fabrications** : le modèle peut produire des réponses incorrectes ou inventées, même si elles semblent plausibles.\n",
    "- **Biais** : ils peuvent refléter les biais présents dans les données d’entraînement (culture, genre, etc.).\n",
    "- **Coût énergétique** : l’entraînement de grands modèles consomme beaucoup de ressources.\n",
    "- **Régulation et éthique** : confidentialité, respect des lois, usage approprié de la technologie.\n",
    "\n",
    "Nous allons maintenant mettre en place notre environnement et faire un premier test de prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.57.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Clé API chargée avec succès !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv \n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis .env\n",
    "load_dotenv()\n",
    "\n",
    "# Récupérer la clé d'API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"Clé API OpenAI non trouvée. Vérifie ton .env !\")\n",
    "\n",
    "print(\"Clé API chargée avec succès !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premier prompt\n",
    "\n",
    "La list des paramètres est accessible dans le [documentation officielle](https://platform.openai.com/docs/api-reference/chat/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse du modèle :\n",
      "Le jour de gloire est arrivé ! C'est le début de \"La Marseillaise\", l'hymne national français. Si tu veux en discuter ou en savoir plus sur son histoire et sa signification, n'hésite pas à\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Premier prompt\n",
    "# ============================\n",
    "\n",
    "# Exemple simple : On veut générer une petite phrase.\n",
    "# On utilise 'chat.completions.create' (nouvelle API).\n",
    "# Le paramètre 'model' peut être par ex. gpt-3.5-turbo ou gpt-4 (ou un autre).\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Allons enfants de la Patrie, \"\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",  # ou gpt-4 si tu y as accès\n",
    "    max_tokens=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notions de base : Tokenisation\n",
    "\n",
    "Lorsque nous envoyons un prompt à un LLM, le texte est d’abord converti en une séquence de **tokens**. \n",
    "- Un token est généralement un morceau de mot, un caractère spécial, ou même un sous-mot.\n",
    "- Plus le nombre de tokens est élevé, plus l’appel au modèle peut coûter cher en ressources (et parfois en argent, selon la facturation).\n",
    "\n",
    "Dans les bibliothèques OpenAI, on peut utiliser la librairie `tiktoken` pour comprendre comment le modèle va découper le texte.\n",
    "\n",
    "**Exercice** : Voyons comment le prompt « Oh say can you see » est découpé en tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens pour text-davinci (indexes) :\n",
      " [5812, 910, 460, 345, 766]\n",
      "\n",
      "Décodage token par token :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n",
      "Liste des tokens pour gpt-4o-mini (indexes) :\n",
      " [18009, 2891, 665, 481, 1921]\n",
      "\n",
      "Décodage token par token :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Tokenisation\n",
    "# ============================\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Choisir l'encodeur en fonction du modèle\n",
    "# (ex: \"gpt-3.5-turbo\", \"text-davinci-003\", etc.)\n",
    "encoder = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "\n",
    "my_text = \"Oh say can you see\"\n",
    "\n",
    "tokens = encoder.encode(my_text)\n",
    "print(\"Liste des tokens pour text-davinci (indexes) :\\n\", tokens)\n",
    "\n",
    "decoded = [encoder.decode([t]) for t in tokens]\n",
    "print(\"\\nDécodage token par token :\")\n",
    "for i, t in enumerate(decoded):\n",
    "    print(f\"Token {i}: '{t}'\")\n",
    "\n",
    "\n",
    "\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "my_text = \"Oh say can you see\"\n",
    "\n",
    "tokens = encoder.encode(my_text)\n",
    "print(\"Liste des tokens pour gpt-4o-mini (indexes) :\\n\", tokens)\n",
    "\n",
    "decoded = [encoder.decode([t]) for t in tokens]\n",
    "print(\"\\nDécodage token par token :\")\n",
    "for i, t in enumerate(decoded):\n",
    "    print(f\"Token {i}: '{t}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de fabrication (ou \"hallucination\")\n",
    "\n",
    "Les modèles de langage peuvent parfois renvoyer des informations inventées ou inexactes. \n",
    "Testons un prompt ambigu ou factuellement faux pour voir comment le modèle réagit.\n",
    "\n",
    "Par exemple : \n",
    "\n",
    "- Lui demander de décrire « la guerre de 2076 sur Mars » \n",
    "- Ou lui demander des détails d’une loi imaginaire\n",
    "\n",
    "Ensuite, observer la réponse et la façon dont le modèle peut inventer des précisions ou admettre ne pas connaître l’information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse du modèle :\n",
      "\n",
      "La guerre de 2076 sur la planète Mars est souvent considérée comme l'un des conflits les plus marquants de l'histoire martienne. Elle a opposé principalement deux grandes puissances : la Coalition Terrienne, composée de plusieurs nations de la Terre unies sous un gouvernement interplanétaire, et la Fédération Martienne, un regroupement de colonies et de factions martiennes cherchant à établir leur indépendance face à la domination terrestre.\n",
      "\n",
      "### Grandes puissances en conflit\n",
      "\n",
      "1. **Coalition Terrienne** : Composée de pays tels que les États-Unis, la Chine, l'Union Européenne et d'autres nations ayant des intérêts économiques et scientifiques sur Mars. La Coalition souhaitait maintenir un contrôle sur les ressources martiennes et\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Fabrication\n",
    "# ============================\n",
    "prompt_fabrication = \"\"\"\n",
    "Décris-moi la célèbre guerre de 2076 sur la planète Mars :\n",
    "- Qui étaient les grandes puissances en conflit ?\n",
    "- Quels traités de paix ont été signés ?\n",
    "\"\"\"\n",
    "\n",
    "response_fabrication = openai.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_fabrication}],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=150,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_fabrication.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion de cette introduction\n",
    "\n",
    "Nous avons abordé :\n",
    "- Les bases de l'IA générative et des LLMs\n",
    "- Comment configurer et appeler rapidement un modèle (OpenAI)\n",
    "- Les notions de tokenisation et d'impact sur la génération\n",
    "- Un aperçu des “fabrications” qu’un modèle peut produire\n",
    "\n",
    "## Pistes pour aller plus loin\n",
    "\n",
    "- Expérimenter différentes **températures** (0.0, 0.7, etc.) pour influencer la créativité de la réponse.\n",
    "- Utiliser l’API **Chat** (chat completions) plutôt que l’API `Completion` pour gérer des dialogues contextuels plus complexes.\n",
    "- Mettre en place de la **RAG** (Retrieval Augmented Generation) pour limiter les hallucinations en donnant au modèle des sources documentaires externes.\n",
    "- Tester la **traduction**, la **rédaction de code** (style Copilot) ou la génération de **contenu marketing**.\n",
    "\n",
    "**Prochaines étapes** dans le cours :\n",
    "1. Approfondir le **prompt engineering** (prompt structuré, chaîne d’invocations, etc.).\n",
    "2. Explorer d’autres types de modèles génératifs (images, audio).\n",
    "3. Étudier en détail les **dimensions éthiques** (biais, utilisation responsable, confidentialité).\n",
    "\n",
    "---\n",
    "\n",
    "Merci d'avoir suivi ce premier Notebook d'introduction !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
