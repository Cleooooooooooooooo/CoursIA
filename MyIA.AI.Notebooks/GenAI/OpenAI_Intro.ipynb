{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests de connections à l'API OpenAI d'Oobabooga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan global du Notebook\n",
    "\n",
    "1. **Introduction générale**  \n",
    "   - Contexte et définitions (IA Générative, LLMs, prompts, etc.)\n",
    "   - Brève présentation des enjeux éthiques et de la responsabilité\n",
    "\n",
    "2. **Premier exemple de code : Vérification d’environnement**  \n",
    "   - Installation/importation des bibliothèques (OpenAI ou Azure OpenAI)\n",
    "   - Test d’un prompt de base\n",
    "\n",
    "3. **Notions de base sur les prompts**  \n",
    "   - Tokens, embeddings, bases de la génération\n",
    "   - Exercices simples : tokenisation et génération\n",
    "\n",
    "4. **Exemple : Fabrications (Hallucinations) et fiabilité**  \n",
    "   - Démonstration d’un prompt volontairement ambigu\n",
    "   - Discussion sur la vérification des faits\n",
    "\n",
    "5. **Conclusion**  \n",
    "   - Rappel des grandes idées\n",
    "   - Activité suggérée (rédaction d’une synthèse ou extension)\n",
    "\n",
    "\n",
    "\n",
    "# Introduction à l'IA Générative\n",
    "\n",
    "Dans ce notebook, nous allons découvrir les bases de l'Intelligence Artificielle Générative. \n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre ce qu’est l’IA Générative et en quoi elle consiste (texte, images, audio…)\n",
    "- Découvrir le fonctionnement des modèles de langage de grande taille (LLMs)\n",
    "- Mettre en pratique de premières expérimentations simples avec les prompts\n",
    "- Aborder brièvement les questions de fiabilité (fabrications / “hallucinations”) et d’éthique\n",
    "\n",
    "## Qu’est-ce que l’IA Générative ?\n",
    "\n",
    "L’IA générative est une branche de l’apprentissage automatique qui **génère** de nouveaux contenus (texte, image, audio, code...) en se basant sur des modèles probabilistes entraînés sur de vastes quantités de données. Les modèles les plus avancés, souvent appelés **Large Language Models** (LLMs), utilisent des réseaux de neurones du type **Transformers** pour prédire et générer des séquences.\n",
    "\n",
    "Exemples concrets :\n",
    "- **ChatGPT** (OpenAI) : génération et compréhension de texte\n",
    "- **Stable Diffusion** : génération d’images\n",
    "- **Audiocraft**, **Whisper** : génération ou transcription audio\n",
    "- **Copilot** (GitHub) : génération de code, complétion et explications\n",
    "\n",
    "## Enjeux et limites\n",
    "- **Hallucinations / Fabrications** : le modèle peut produire des réponses incorrectes ou inventées, même si elles semblent plausibles.\n",
    "- **Biais** : ils peuvent refléter les biais présents dans les données d’entraînement (culture, genre, etc.).\n",
    "- **Coût énergétique** : l’entraînement de grands modèles consomme beaucoup de ressources.\n",
    "- **Régulation et éthique** : confidentialité, respect des lois, usage approprié de la technologie.\n",
    "\n",
    "Nous allons maintenant mettre en place notre environnement et faire un premier test de prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv \n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis .env\n",
    "load_dotenv()\n",
    "\n",
    "# Récupérer la clé d'API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"Clé API OpenAI non trouvée. Vérifie ton .env !\")\n",
    "\n",
    "print(\"Clé API chargée avec succès !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Premier prompt\n",
    "# ============================\n",
    "\n",
    "# Exemple simple : On veut générer une petite phrase.\n",
    "# On utilise 'chat.completions.create' (nouvelle API).\n",
    "# Le paramètre 'model' peut être par ex. gpt-3.5-turbo ou gpt-4 (ou un autre).\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Oh say can you see\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",  # ou gpt-4 si tu y as accès\n",
    "    max_tokens=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notions de base : Tokenisation\n",
    "\n",
    "Lorsque nous envoyons un prompt à un LLM, le texte est d’abord converti en une séquence de **tokens**. \n",
    "- Un token est généralement un morceau de mot, un caractère spécial, ou même un sous-mot.\n",
    "- Plus le nombre de tokens est élevé, plus l’appel au modèle peut coûter cher en ressources (et parfois en argent, selon la facturation).\n",
    "\n",
    "Dans les bibliothèques OpenAI, on peut utiliser la librairie `tiktoken` pour comprendre comment le modèle va découper le texte.\n",
    "\n",
    "**Exercice** : Voyons comment le prompt « Oh say can you see » est découpé en tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Tokenisation\n",
    "# ============================\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Choisir l'encodeur en fonction du modèle\n",
    "# (ex: \"gpt-3.5-turbo\", \"text-davinci-003\", etc.)\n",
    "encoder = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "\n",
    "my_text = \"Oh say can you see\"\n",
    "\n",
    "tokens = encoder.encode(my_text)\n",
    "print(\"Liste des tokens (indexes) :\\n\", tokens)\n",
    "\n",
    "decoded = [encoder.decode([t]) for t in tokens]\n",
    "print(\"\\nDécodage token par token :\")\n",
    "for i, t in enumerate(decoded):\n",
    "    print(f\"Token {i}: '{t}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de fabrication (ou \"hallucination\")\n",
    "\n",
    "Les modèles de langage peuvent parfois renvoyer des informations inventées ou inexactes. \n",
    "Testons un prompt ambigu ou factuellement faux pour voir comment le modèle réagit.\n",
    "\n",
    "Par exemple : \n",
    "\n",
    "- Lui demander de décrire « la guerre de 2076 sur Mars » \n",
    "- Ou lui demander des détails d’une loi imaginaire\n",
    "\n",
    "Ensuite, observer la réponse et la façon dont le modèle peut inventer des précisions ou admettre ne pas connaître l’information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Fabrication\n",
    "# ============================\n",
    "prompt_fabrication = \"\"\"\n",
    "Décris-moi la célèbre guerre de 2076 sur la planète Mars :\n",
    "- Qui étaient les grandes puissances en conflit ?\n",
    "- Quels traités de paix ont été signés ?\n",
    "\"\"\"\n",
    "\n",
    "response_fabrication = openai.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_fabrication}],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=150,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_fabrication.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion de cette introduction\n",
    "\n",
    "Nous avons abordé :\n",
    "- Les bases de l'IA générative et des LLMs\n",
    "- Comment configurer et appeler rapidement un modèle (OpenAI)\n",
    "- Les notions de tokenisation et d'impact sur la génération\n",
    "- Un aperçu des “fabrications” qu’un modèle peut produire\n",
    "\n",
    "## Pistes pour aller plus loin\n",
    "\n",
    "- Expérimenter différentes **températures** (0.0, 0.7, etc.) pour influencer la créativité de la réponse.\n",
    "- Utiliser l’API **Chat** (chat completions) plutôt que l’API `Completion` pour gérer des dialogues contextuels plus complexes.\n",
    "- Mettre en place de la **RAG** (Retrieval Augmented Generation) pour limiter les hallucinations en donnant au modèle des sources documentaires externes.\n",
    "- Tester la **traduction**, la **rédaction de code** (style Copilot) ou la génération de **contenu marketing**.\n",
    "\n",
    "**Prochaines étapes** dans le cours :\n",
    "1. Approfondir le **prompt engineering** (prompt structuré, chaîne d’invocations, etc.).\n",
    "2. Explorer d’autres types de modèles génératifs (images, audio).\n",
    "3. Étudier en détail les **dimensions éthiques** (biais, utilisation responsable, confidentialité).\n",
    "\n",
    "---\n",
    "\n",
    "Merci d'avoir suivi ce premier Notebook d'introduction !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
