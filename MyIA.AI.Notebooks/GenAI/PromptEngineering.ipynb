{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering : Advanced Prompting avec OpenAI\n",
    "\n",
    "Dans ce notebook, nous allons tester différentes techniques avancées de **prompt engineering**:\n",
    "- **Zero-shot prompting**\n",
    "- **Few-shot prompting**\n",
    "- **Chain-of-thought** (CoT)\n",
    "- **Self-refine** (ou auto-amélioration)\n",
    "\n",
    "Nous utiliserons la **nouvelle API** de la bibliothèque `openai` (>=1.0.0) via la classe `OpenAI` et ses méthodes de chat (`client.chat.completions.create`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.57.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# On suppose que ton .env contient :\n",
    "# OPENAI_API_KEY=sk-xxxxxx\n",
    "# (ou autre variable si tu utilises Azure)\n",
    "#\n",
    "# Récupère la clé d'API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Clé API introuvable. Vérifie ton fichier .env.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé avec succès !\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Client OpenAI\n",
    "# ============================\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Pour l'exemple, on définit le modèle par défaut\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "# Instanciation du client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    # Tu peux configurer d'autres options si besoin\n",
    ")\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappel des différences entre Zero-shot, Few-shot, Chain-of-thought et Self-refine\n",
    "\n",
    "1. **Zero-shot Prompting**  \n",
    "   - Aucune instruction ou exemple préalable (à part la demande de l'utilisateur).  \n",
    "   - Simple et direct, mais parfois moins précis ou cohérent.\n",
    "\n",
    "2. **Few-shot Prompting**  \n",
    "   - Fournir quelques exemples “input → output” pour guider la réponse.  \n",
    "   - Permet de **spécifier le format**, le style, ou le contenu souhaité.  \n",
    "   - Améliore significativement la qualité des réponses sur des tâches complexes.\n",
    "\n",
    "3. **Chain-of-thought (CoT)**  \n",
    "   - On **incite** le modèle à détailler son raisonnement étape par étape.  \n",
    "   - Souvent utile pour des questions de logique, mathématiques, programmation ou raisonnement complexe.  \n",
    "   - Peut **augmenter** la cohérence et la justesse de la réponse (mais attention à ne pas divulguer ces “étapes” si elles sont confidentielles).\n",
    "\n",
    "4. **Self-refine**  \n",
    "   - Demander au modèle de s’auto-critiquer puis de proposer une réponse améliorée.  \n",
    "   - Mise en œuvre en plusieurs appels (réponse initiale, re-demande d’analyse, ré-énoncé final).  \n",
    "   - Intéressant pour du code, des textes longs, ou des situations nécessitant un contrôle qualité.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Prompt ===\n",
      "Prompt: Donne-moi 3 idées de recettes végétariennes à base de tomates.\n",
      "\n",
      "Réponse du modèle :\n",
      "\n",
      "Bien sûr ! Voici trois idées de recettes végétariennes à base de tomates :\n",
      "\n",
      "### 1. **Tartine de tomates et avocat**\n",
      "**Ingrédients :**\n",
      "- Pain complet ou pain au levain\n",
      "- Tomates mûres\n",
      "- Avocat\n",
      "- Basilic frais\n",
      "- Huile d'olive\n",
      "- Sel et poivre\n",
      "\n",
      "**Instructions :**\n",
      "1. Faites griller les tranches de pain.\n",
      "2. Pendant ce temps, coupez les tomates et l'avocat en tranches.\n",
      "3. Disposez les tranches d'avocat sur le pain grillé, puis ajoutez les tranches de tomates par-dessus.\n",
      "4. Arrosez d'un filet d'huile d'olive, ajoutez du sel, du poivre et quelques feuilles de basilic frais pour garnir.\n",
      "5. Servez immédiatement.\n",
      "\n",
      "### 2. **Ratatouille**\n",
      "**Ingrédients :**\n",
      "- 2 tomates\n",
      "- 1 courgette\n",
      "- \n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Zero-shot\n",
    "# ============================\n",
    "\n",
    "prompt_1 = \"Donne-moi 3 idées de recettes végétariennes à base de tomates.\"\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_1}\n",
    "    ],\n",
    "    # Contrôle du style\n",
    "    max_tokens=200,\n",
    "    temperature=0.7  # plus la température est haute, plus c'est créatif\n",
    ")\n",
    "\n",
    "print(\"=== Zero-shot Prompt ===\")\n",
    "print(f\"Prompt: {prompt_1}\\n\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, pas d’exemples ni d’instructions détaillées, on se contente d’un prompt direct.\n",
    "\n",
    "\n",
    "\n",
    "###  Exemple Few-shot prompting (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exemple Few-shot (e-mail professionnel) ===\n",
      "Sujet: Changement de planning et invitation à une réunion de suivi\n",
      "\n",
      "Bonjour [Nom du Collaborateur],\n",
      "\n",
      "Je souhaite vous informer d'un changement dans notre planning initial. En raison de [raison du changement], nous devons ajuster certaines de nos échéances.\n",
      "\n",
      "Pour discuter des implications de ce changement et de la manière dont nous pouvons nous organiser au mieux, je vous invite à une réunion de suivi le [date] à [heure]. Nous pourrons aborder les points essentiels et répondre à toutes vos questions.\n",
      "\n",
      "Merci de me confirmer votre disponibilité.\n",
      "\n",
      "Cordialement,\n",
      "\n",
      "[Votre Nom]  \n",
      "[Votre Poste]  \n",
      "[Votre Société]  \n",
      "[Vos Coordonnées]\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Few-shot supplémentaire\n",
    "# ============================\n",
    "\n",
    "few_shot_prompt_2 = \"\"\"\n",
    "Tu es un assistant spécialisé en rédaction d'e-mails professionnels.\n",
    "Voici quelques exemples de style :\n",
    "\n",
    "Exemple 1:\n",
    "Q: Rédige un e-mail pour informer un client d'un retard de livraison\n",
    "A: \n",
    "Sujet: Information concernant le retard de votre livraison\n",
    "\n",
    "Bonjour [Nom du Client],\n",
    "\n",
    "Nous tenions à vous informer que votre commande #1234 a pris du retard...\n",
    "[...suite du mail...]\n",
    "\n",
    "Exemple 2:\n",
    "Q: Envoie un e-mail de remerciement pour un entretien d'embauche\n",
    "A:\n",
    "Sujet: Remerciements suite à notre entretien\n",
    "\n",
    "Bonjour [Nom du Contact],\n",
    "\n",
    "Je tiens à vous remercier pour le temps que vous m'avez accordé...\n",
    "[...suite du mail...]\n",
    "\n",
    "Maintenant, voici ma demande:\n",
    "\n",
    "Q: Écris un e-mail pour informer un collaborateur d'un changement de planning et l'inviter à une réunion de suivi.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response_few_shot_2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,  # ex. \"gpt-4o-mini\"\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt_2}\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(\"=== Exemple Few-shot (e-mail professionnel) ===\")\n",
    "print(response_few_shot_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous donnons au modèle **deux exemples** de questions/réponses avant la **véritable question**. Cela oriente le style et le contexte.\n",
    "\n",
    "\n",
    "\n",
    "###  7 : Exemple Chain-of-thought (Code)\n",
    "\n",
    "On va demander un **calcul** simple, en guidant le modèle à réfléchir pas à pas :\n",
    "\n",
    "\n",
    "\n",
    "- **Masquer le raisonnement si nécessaire** :  \n",
    "  Parfois, on ne souhaite pas afficher au client final les étapes du raisonnement. Il existe des techniques pour “cacher” ce raisonnement ou n’afficher qu’un résumé. Par exemple :  \n",
    "  1. Dans le prompt, on peut demander : “Don’t reveal your chain-of-thought. Provide only the final concise answer to the user.”  \n",
    "  2. Ou effectuer un 2e appel API où l’on transmet l’enchaînement de pensées, mais on n'affiche que la conclusion.\n",
    "\n",
    "- **Adapter la température** :  \n",
    "  Pour un problème logique ou mathématique, une température trop élevée peut introduire des dérives ou des incohérences. Une température entre 0.0 et 0.3 est souvent recommandée pour les questions de calcul.\n",
    "\n",
    "- **Chain-of-thought partiel** :  \n",
    "  On peut encourager un raisonnement **intermédiaire** (quelques étapes clés) au lieu d’un raisonnement hyper détaillé, afin d’éviter que le texte devienne trop long ou difficile à comprendre.\n",
    "\n",
    "- **Expliquer la démarche** :  \n",
    "  On peut terminer par un résumé du raisonnement en 2-3 phrases, pour rendre la solution plus lisible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain-of-thought Prompt ===\n",
      "Réponse du modèle (avec raisonnement) :\n",
      "\n",
      "Pour résoudre le problème, suivons les étapes une par une :\n",
      "\n",
      "1. **Alice commence avec 5 pommes.**\n",
      "   - Pommes d'Alice : 5\n",
      "\n",
      "2. **Elle en jette 2.**\n",
      "   - Pommes d'Alice après avoir jeté 2 pommes : 5 - 2 = 3\n",
      "   - Pommes d'Alice : 3\n",
      "\n",
      "3. **Elle en donne 1 à Bob.**\n",
      "   - Pommes d'Alice après avoir donné 1 pomme à Bob : 3 - 1 = 2\n",
      "   - Pommes d'Alice : 2\n",
      "\n",
      "4. **Bob lui rend 1 pomme.**\n",
      "   - Pommes d'Alice après que Bob lui a rendu 1 pomme : 2 + 1 = 3\n",
      "   - Pommes d'Alice : 3\n",
      "\n",
      "Donc, à la fin, Alice a **3 pommes**.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Chain-of-thought\n",
    "# ============================\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "\n",
    "Explique ton raisonnement étape par étape, puis donne la réponse finale.\n",
    "\"\"\"\n",
    "\n",
    "response_3 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.2  # on réduit la température pour moins de fantaisie\n",
    ")\n",
    "\n",
    "print(\"=== Chain-of-thought Prompt ===\")\n",
    "print(\"Réponse du modèle (avec raisonnement) :\\n\")\n",
    "print(response_3.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On demande explicitement « explique ton raisonnement ». Cela **n’oblige** pas le modèle à le faire, mais en pratique, GPT-4o-mini (ou tout modèle qui gère le CoT) fournit souvent une solution pas-à-pas.\n",
    "\n",
    "---\n",
    "\n",
    "###  8 : Exemple Self-refine (Code)\n",
    "\n",
    "L’idée : on fait **une première demande** (première réponse) et ensuite **on redemande** au modèle de s’auto-corriger.\n",
    "\n",
    "#### 8a. Premier prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (1) : Code buggy ===\n",
      "\n",
      "Voici une fonction Python qui calcule la somme d'une liste, avec un bug volontaire :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    total = 0\n",
      "    for element in liste:\n",
      "        total += element  # Bug : on ajoute toujours 1 à total\n",
      "    return total + 1  # Bug : on ajoute 1 à la somme finale\n",
      "\n",
      "# Exemple d'utilisation\n",
      "ma_liste = [1, 2, 3, 4]\n",
      "print(somme_liste(ma_liste))  # Cela devrait afficher 10, mais affichera 11 à cause des bugs\n",
      "```\n",
      "\n",
      "Dans cette fonction, le bug réside dans le fait que la somme finale est augmentée de 1 et que le code devrait uniquement additionner les éléments de la liste.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8a : Self-refine Step 1\n",
    "# ============================\n",
    "\n",
    "prompt_sr1 = \"\"\"\n",
    "Ecris une courte fonction Python pour calculer la somme d'une liste. \n",
    "Ajoute un bug volontaire dans le code. \n",
    "\"\"\"\n",
    "\n",
    "response_sr1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr1}],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "buggy_code = response_sr1.choices[0].message.content\n",
    "\n",
    "print(\"=== Self-refine (1) : Code buggy ===\\n\")\n",
    "print(buggy_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8b. Self-critique et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (2) : Correction ===\n",
      "\n",
      "Bien sûr ! Analysons le code que vous avez fourni.\n",
      "\n",
      "### Analyse du code\n",
      "\n",
      "La fonction `somme_liste` est censée calculer la somme des éléments d'une liste. Cependant, il y a deux bugs :\n",
      "\n",
      "1. **Ajout incorrect dans la boucle** : Le commentaire indique qu'il y a un bug qui consiste à ajouter toujours 1 à `total`, mais en réalité, le code ajoute chaque élément de la liste à `total`. Il n'y a pas de bug dans cette ligne. Le commentaire est trompeur.\n",
      "   \n",
      "2. **Ajout incorrect à la somme finale** : La ligne `return total + 1` ajoute 1 à la somme finale, ce qui est incorrect. La fonction devrait simplement retourner `total`.\n",
      "\n",
      "### Correctif\n",
      "\n",
      "Pour corriger ces bugs, nous allons simplement supprimer l'ajout de 1 à la somme finale. Voici la version corrigée du code :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    total = 0\n",
      "    for element in liste:\n",
      "        total += element  # Ajoute chaque élément à total\n",
      "    return total  # Retourne simplement la somme\n",
      "\n",
      "# Exemple d'utilisation\n",
      "ma_liste = [1, 2, 3, 4]\n",
      "print(somme_liste(ma_liste))  # Cela affichera maintenant 10\n",
      "```\n",
      "\n",
      "### Version améliorée\n",
      "\n",
      "Nous pouvons également améliorer la fonction en utilisant la fonction intégrée `sum()` de Python, qui est plus concise et optimisée. Voici une version améliorée :\n",
      "\n",
      "```python\n",
      "def somme_liste(liste):\n",
      "    return sum(liste)  # Utilise la fonction intégrée sum pour calculer la somme\n",
      "\n",
      "# Exemple d'utilisation\n",
      "ma_liste = [1, 2, 3, 4]\n",
      "print(somme_liste(ma_liste))  # Cela affichera 10\n",
      "```\n",
      "\n",
      "### Explication de la correction\n",
      "\n",
      "- Nous avons supprimé l'ajout de \n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8b : Self-refine Step 2\n",
    "# ============================\n",
    "\n",
    "prompt_sr2 = f\"\"\"\n",
    "Voici un code Python qui contient un bug:\n",
    "\n",
    "{buggy_code}\n",
    "\n",
    "Peux-tu l'analyser, détecter le bug, proposer un correctif et une version améliorée du code ? \n",
    "Explique la correction.\n",
    "\"\"\"\n",
    "\n",
    "response_sr2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr2}],\n",
    "    max_tokens=400,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine (2) : Correction ===\\n\")\n",
    "print(response_sr2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine avec 'developer role' ===\n",
      "Voici une fonction Python qui calcule la factorielle d'un nombre entier. Je vais également vérifier et corriger d'éventuels bugs.\n",
      "\n",
      "```python\n",
      "def factorielle(n):\n",
      "    if not isinstance(n, int):\n",
      "        raise ValueError(\"L'entrée doit être un entier.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"La factorielle n'est pas définie pour les nombres négatifs.\")\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    resultat = 1\n",
      "    for i in range(2, n + 1):\n",
      "        resultat *= i\n",
      "    return resultat\n",
      "```\n",
      "\n",
      "### Améliorations et vérifications :\n",
      "\n",
      "1. **Validation de l'entrée** : J'ai ajouté une vérification pour s'assurer que l'entrée est un entier et qu'elle n'est pas négative.\n",
      "2. **Cas de base** : Les cas de base pour `0!` et `1!` sont correctement gérés.\n",
      "3. **Utilisation d'une boucle** : La fonction utilise une boucle pour calculer la factorielle, ce qui est efficace pour des valeurs raisonnables de `n`.\n",
      "\n",
      "### Test de la fonction :\n",
      "\n",
      "Pour s'assurer que la fonction fonctionne correctement, voici quelques tests :\n",
      "\n",
      "```python\n",
      "print(factorielle(5))  # 120\n",
      "print(factorielle(0))  # 1\n",
      "print(factorielle(1))  # 1\n",
      "print(factorielle(10)) # \n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule X (NOUVELLE) : Self-refine avec developer role\n",
    "# ============================\n",
    "\n",
    "messages_sr = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": (\n",
    "            \"You are a self-improving coding assistant. Whenever you provide code, \"\n",
    "            \"you will automatically search for potential bugs or improvements \"\n",
    "            \"and refine your output.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Écris une fonction Python qui calcule la factorielle d'un nombre entier. \"\n",
    "            \"Ensuite, relis-toi et corrige d'éventuels bugs.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "response_self_refine = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages_sr,\n",
    "    max_tokens=300,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine avec 'developer role' ===\")\n",
    "print(response_self_refine.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on utilise la première réponse pour nourrir le second prompt, demandant au modèle de **critiquer** et **améliorer** la réponse initiale.\n",
    "\n",
    "---\n",
    "\n",
    "### Cellule 9 : Interactive Prompt (Code)\n",
    "\n",
    "Enfin, on peut proposer une **cellule interactive** : l’utilisateur peut saisir un prompt, et on envoie la requête au modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de l'interaction.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif\n",
    "# ============================\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Fin de l'interaction.\")\n",
    "        break\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\":\"user\",\"content\":user_input}],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Réponse du modèle ===\")\n",
    "    print(resp.choices[0].message.content)\n",
    "    print(\"---------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de l'interaction.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif avec mémoire de chat\n",
    "# ============================\n",
    "user_input = \"\";\n",
    "current_messages=[]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Fin de l'interaction.\")\n",
    "        break\n",
    "    print(\"\\n=== message de l'utilisateur ===\")\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages = current_messages + [{\"role\":\"user\",\"content\":user_input}],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Réponse du modèle ===\")\n",
    "    assistant_message = resp.choices[0].message.content\n",
    "    print(assistant_message)\n",
    "    print(\"---------------------------------------------------\\n\")\n",
    "    current_messages.append({\"role\":\"assistant\",\"content\":assistant_message})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, tu peux saisir n’importe quel prompt, et tu verras la réponse du modèle.  \n",
    "Tape `exit` pour quitter la boucle.\n",
    "\n",
    "----\n",
    "\n",
    "## Conclusion et Ressources Supplémentaires\n",
    "\n",
    "Dans ce notebook, nous avons approfondi diverses techniques de **prompt engineering** : \n",
    "- Zero-shot  \n",
    "- Few-shot  \n",
    "- Chain-of-thought  \n",
    "- Self-refine  \n",
    "- Interactions multi-messages (avec les rôles `system`, `developer`, `user`, `assistant`)  \n",
    "\n",
    "### Ressources conseillées\n",
    "- [**OpenAI Cookbook**](https://github.com/openai/openai-cookbook) : Recettes et astuces pour résoudre des problèmes concrets (prompt engineering, RAG, etc.).  \n",
    "- [**Prompt Engineering Guide**](https://www.promptingguide.ai/) : Conseils de rédaction de prompts, cas d’usages, bonnes pratiques.  \n",
    "- [**Chaine d’outils** (LangChain, LlamaIndex, etc.)](https://github.com/hwchase17/langchain) : Facilite la création de pipelines complexes (RAG, function calling, mémoire de conversation).  \n",
    "\n",
    "> **Idées d’exercices**  \n",
    "> 1. Adapter la technique Few-shot à d’autres cas (e.g., Q&R sur la finance, la santé ou le marketing).  \n",
    "> 2. Utiliser la Self-refine pour générer un texte marketing, puis le réécrire en style “plus formel” ou “plus humoristique”.  \n",
    "> 3. Tester la **combinaison** de techniques : un prompt Few-shot + Chain-of-thought + un re-run Self-refine.\n",
    "\n",
    "---\n",
    "\n",
    "Merci d'avoir suivi ce notebook sur le **prompt engineering avancé** !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
