{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Prompting avec OpenAI\n",
    "\n",
    "Dans ce notebook, nous allons tester différentes techniques avancées de **prompt engineering**:\n",
    "- **Zero-shot prompting**\n",
    "- **Few-shot prompting**\n",
    "- **Chain-of-thought** (CoT)\n",
    "- **Self-refine** (ou auto-amélioration)\n",
    "\n",
    "Nous utiliserons la **nouvelle API** de la bibliothèque `openai` (>=1.0.0) via la classe `OpenAI` et ses méthodes de chat (`client.chat.completions.create`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.57.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# On suppose que ton .env contient :\n",
    "# OPENAI_API_KEY=sk-xxxxxx\n",
    "# (ou autre variable si tu utilises Azure)\n",
    "#\n",
    "# Récupère la clé d'API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Clé API introuvable. Vérifie ton fichier .env.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé avec succès !\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Client OpenAI\n",
    "# ============================\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Pour l'exemple, on définit le modèle par défaut\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "# Instanciation du client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    # Tu peux configurer d'autres options si besoin\n",
    ")\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rappels sur le Prompt Engineering avancé\n",
    "\n",
    "Plusieurs techniques de `prompt engineering` permettent d'améliorer les réponses d'un LLM:\n",
    "\n",
    "1. **Zero-shot prompting**  \n",
    "   On formule une simple question (ou instruction), sans donner d'exemples, et le modèle déduit la tâche.\n",
    "\n",
    "2. **Few-shot prompting**  \n",
    "   On fournit quelques exemples (input → output) pour guider le modèle vers le style, le format ou la logique attendus.\n",
    "\n",
    "3. **Chain-of-thought (CoT)**  \n",
    "   On incite le modèle à décomposer sa réflexion par étapes. Cela améliore souvent la justesse sur des problèmes complexes (calculs, raisonnement).\n",
    "\n",
    "4. **Self-refine**  \n",
    "   On demande au modèle de s'auto-corriger (self-critique) puis de proposer une version améliorée de sa réponse.\n",
    "\n",
    "Nous allons montrer quelques brefs exemples !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Prompt ===\n",
      "Prompt: Donne-moi 3 idées de recettes végétariennes à base de tomates.\n",
      "\n",
      "Réponse du modèle :\n",
      "\n",
      "Bien sûr ! Voici trois idées de recettes végétariennes à base de tomates :\n",
      "\n",
      "### 1. **Tarte à la tomate et au fromage de chèvre**\n",
      "**Ingrédients :**\n",
      "- Pâte brisée\n",
      "- Tomates fraîches (variété de votre choix)\n",
      "- Fromage de chèvre\n",
      "- Herbes de Provence\n",
      "- Huile d'olive\n",
      "- Sel et poivre\n",
      "\n",
      "**Instructions :**\n",
      "1. Préchauffez le four à 180°C.\n",
      "2. Étalez la pâte brisée dans un moule à tarte et piquez le fond avec une fourchette.\n",
      "3. Coupez les tomates en rondelles et disposez-les sur la pâte.\n",
      "4. Émiettez le fromage de chèvre par-dessus.\n",
      "5. Arrosez d'un filet d'huile d'olive, saupoudrez d'herbes de Provence, de sel et de poivre.\n",
      "6. Enfournez pendant 30 minutes ou jusqu\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Zero-shot\n",
    "# ============================\n",
    "\n",
    "prompt_1 = \"Donne-moi 3 idées de recettes végétariennes à base de tomates.\"\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_1}\n",
    "    ],\n",
    "    # Contrôle du style\n",
    "    max_tokens=200,\n",
    "    temperature=0.7  # plus la température est haute, plus c'est créatif\n",
    ")\n",
    "\n",
    "print(\"=== Zero-shot Prompt ===\")\n",
    "print(f\"Prompt: {prompt_1}\\n\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, pas d’exemples ni d’instructions détaillées, on se contente d’un prompt direct.\n",
    "\n",
    "\n",
    "\n",
    "###  Exemple Few-shot prompting (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Few-shot Prompt ===\n",
      "Réponse du modèle :\n",
      "\n",
      "- Ratatouille aux légumes du soleil : un mélange savoureux d'aubergines, de courgettes, de poivrons et de tomates, mijoté avec des herbes de Provence.\n",
      "  \n",
      "- Tomates farcies aux quinoa et légumes : des tomates creusées et remplies d'un mélange de quinoa, de courgettes, d'oignons et d'épices, puis cuites au four.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 6 : Few-shot\n",
    "# ============================\n",
    "\n",
    "few_shot_prompt = \"\"\"\n",
    "Tu es un assistant culinaire spécialisé en recettes végétariennes.\n",
    "Voici des exemples :\n",
    "\n",
    "Exemple 1:\n",
    "Q: Quelles idées de salade d'été me proposes-tu ?\n",
    "A: - Salade de quinoa et tomates cerises\n",
    "   - Salade de lentilles aux oignons rouges\n",
    "   - ...\n",
    "\n",
    "Exemple 2:\n",
    "Q: Je veux cuisiner des champignons, as-tu une idée ?\n",
    "A: - Poêlée de champignons, ail et persil\n",
    "   - ...\n",
    "\n",
    "Maintenant, voici ma question:\n",
    "\n",
    "Q: Propose-moi 2 plats végétariens sans gluten, si possible avec des tomates.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt}\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=== Few-shot Prompt ===\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous donnons au modèle **deux exemples** de questions/réponses avant la **véritable question**. Cela oriente le style et le contexte.\n",
    "\n",
    "\n",
    "\n",
    "###  7 : Exemple Chain-of-thought (Code)\n",
    "\n",
    "On va demander un **calcul** simple, en guidant le modèle à réfléchir pas à pas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain-of-thought Prompt ===\n",
      "Réponse du modèle (avec raisonnement) :\n",
      "\n",
      "Pour résoudre ce problème, nous allons suivre les étapes une par une.\n",
      "\n",
      "1. **Alice commence avec 5 pommes.**\n",
      "   - Pommes d'Alice : 5\n",
      "\n",
      "2. **Elle en jette 2.**\n",
      "   - Pommes d'Alice après avoir jeté 2 : 5 - 2 = 3\n",
      "   - Pommes d'Alice : 3\n",
      "\n",
      "3. **Elle en donne 1 à Bob.**\n",
      "   - Pommes d'Alice après avoir donné 1 à Bob : 3 - 1 = 2\n",
      "   - Pommes d'Alice : 2\n",
      "\n",
      "4. **Bob lui rend ensuite 1 pomme.**\n",
      "   - Pommes d'Alice après que Bob lui a rendu 1 : 2 + 1 = 3\n",
      "   - Pommes d'Alice : 3\n",
      "\n",
      "En résumé, après toutes ces actions, Alice a 3 pommes à la fin.\n",
      "\n",
      "**Réponse finale : Alice a\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Chain-of-thought\n",
    "# ============================\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "\n",
    "Explique ton raisonnement étape par étape, puis donne la réponse finale.\n",
    "\"\"\"\n",
    "\n",
    "response_3 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.2  # on réduit la température pour moins de fantaisie\n",
    ")\n",
    "\n",
    "print(\"=== Chain-of-thought Prompt ===\")\n",
    "print(\"Réponse du modèle (avec raisonnement) :\\n\")\n",
    "print(response_3.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On demande explicitement « explique ton raisonnement ». Cela **n’oblige** pas le modèle à le faire, mais en pratique, GPT-4o-mini (ou tout modèle qui gère le CoT) fournit souvent une solution pas-à-pas.\n",
    "\n",
    "---\n",
    "\n",
    "###  8 : Exemple Self-refine (Code)\n",
    "\n",
    "L’idée : on fait **une première demande** (première réponse) et ensuite **on redemande** au modèle de s’auto-corriger.\n",
    "\n",
    "#### 8a. Premier prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (1) : Code buggy ===\n",
      "\n",
      "Voici une courte fonction Python qui calcule la somme d'une liste, avec un bug volontaire inclus :\n",
      "\n",
      "```python\n",
      "def somme_liste(lst):\n",
      "    total = 0\n",
      "    for i in range(len(lst)):\n",
      "        total += lst[i]  # Bug : utilisation de 'range(len(lst))' au lieu de 'for i in lst'\n",
      "    return total\n",
      "\n",
      "# Exemple d’utilisation\n",
      "ma_liste = [1, 2, 3, 4, 5]\n",
      "print(somme_liste(ma_liste))  # Cela ne donnera pas la somme correcte\n",
      "```\n",
      "\n",
      "Dans cette version, le bug est que nous utilisons `range(len(lst))` pour accéder aux éléments de la liste au lieu d'itérer directement sur les éléments de la liste avec `for i in lst`.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8a : Self-refine Step 1\n",
    "# ============================\n",
    "\n",
    "prompt_sr1 = \"\"\"\n",
    "Ecris une courte fonction Python pour calculer la somme d'une liste. \n",
    "Ajoute un bug volontaire dans le code. \n",
    "\"\"\"\n",
    "\n",
    "response_sr1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr1}],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "buggy_code = response_sr1.choices[0].message.content\n",
    "\n",
    "print(\"=== Self-refine (1) : Code buggy ===\\n\")\n",
    "print(buggy_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8b. Self-critique et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (2) : Correction ===\n",
      "\n",
      "Analysons le code Python fourni et identifions le bug.\n",
      "\n",
      "### Analyse du code\n",
      "\n",
      "La fonction `somme_liste` est censée calculer la somme des éléments d'une liste. Le code utilise une boucle `for` avec `range(len(lst))`, ce qui signifie qu'il itère sur les indices de la liste plutôt que sur les éléments eux-mêmes. Cependant, dans ce cas, le code fonctionne correctement et renvoie la somme des éléments de la liste, car `lst[i]` accède bien aux éléments de la liste en utilisant les indices.\n",
      "\n",
      "### Bug identifié\n",
      "\n",
      "Le bug mentionné dans le code n'est pas un bug fonctionnel, car le code renvoie effectivement la somme correcte. Cependant, il est vrai que l'utilisation de `range(len(lst))` est moins idiomatique en Python. La manière plus pythonique d'itérer sur les éléments d'une liste est d'utiliser `for i in lst`, ce qui rend le code plus lisible et plus concis.\n",
      "\n",
      "### Correctif et version améliorée\n",
      "\n",
      "Voici la version corrigée et améliorée du code :\n",
      "\n",
      "```python\n",
      "def somme_liste(lst):\n",
      "    total = 0\n",
      "    for i in lst:  # Itération directe sur les éléments de la liste\n",
      "        total += i\n",
      "    return total\n",
      "\n",
      "# Exemple d’utilisation\n",
      "ma_liste = [1, 2, 3, 4, 5]\n",
      "print(somme_liste(ma_liste))  # Cela donnera la somme correcte\n",
      "```\n",
      "\n",
      "### Explication de la correction\n",
      "\n",
      "1. **Itération directe** : En utilisant `for i in lst`, nous itérons directement sur les éléments de la liste, ce qui est plus lisible et évite d'accéder aux éléments par leurs indices. Cela rend le code plus clair et plus facile à comprendre.\n",
      "\n",
      "2. **Simplicité** : Cette approche réduit également le risque d'erreurs potentielles liées à l'indexation, comme\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8b : Self-refine Step 2\n",
    "# ============================\n",
    "\n",
    "prompt_sr2 = f\"\"\"\n",
    "Voici un code Python qui contient un bug:\n",
    "\n",
    "{buggy_code}\n",
    "\n",
    "Peux-tu l'analyser, détecter le bug, proposer un correctif et une version améliorée du code ? \n",
    "Explique la correction.\n",
    "\"\"\"\n",
    "\n",
    "response_sr2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr2}],\n",
    "    max_tokens=400,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine (2) : Correction ===\\n\")\n",
    "print(response_sr2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on utilise la première réponse pour nourrir le second prompt, demandant au modèle de **critiquer** et **améliorer** la réponse initiale.\n",
    "\n",
    "---\n",
    "\n",
    "### Cellule 9 : Interactive Prompt (Code)\n",
    "\n",
    "Enfin, on peut proposer une **cellule interactive** : l’utilisateur peut saisir un prompt, et on envoie la requête au modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Réponse du modèle ===\n",
      "Salut ! Comment puis-je vous aider aujourd'hui ?\n",
      "---------------------------------------------------\n",
      "\n",
      "Fin de l'interaction.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif\n",
    "# ============================\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Fin de l'interaction.\")\n",
    "        break\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\":\"user\",\"content\":user_input}],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Réponse du modèle ===\")\n",
    "    print(resp.choices[0].message.content)\n",
    "    print(\"---------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, tu peux saisir n’importe quel prompt, et tu verras la réponse du modèle.  \n",
    "Tape `exit` pour quitter la boucle.\n",
    "\n",
    "----\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Dans ce Notebook, nous avons :\n",
    "\n",
    "- configuré la nouvelle API `openai >= 1.0.0` (avec `client.chat.completions.create(...)`),\n",
    "- utilisé plusieurs techniques de **prompt engineering avancé** : \n",
    "  - zero-shot, \n",
    "  - few-shot, \n",
    "  - chain-of-thought,\n",
    "  - self-refine,\n",
    "  - un prompt interactif final."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
