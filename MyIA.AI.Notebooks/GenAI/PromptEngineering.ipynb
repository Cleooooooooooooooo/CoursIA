{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Prompting avec OpenAI\n",
    "\n",
    "Dans ce notebook, nous allons tester différentes techniques avancées de **prompt engineering**:\n",
    "- **Zero-shot prompting**\n",
    "- **Few-shot prompting**\n",
    "- **Chain-of-thought** (CoT)\n",
    "- **Self-refine** (ou auto-amélioration)\n",
    "\n",
    "Nous utiliserons la **nouvelle API** de la bibliothèque `openai` (>=1.0.0) via la classe `OpenAI` et ses méthodes de chat (`client.chat.completions.create`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.57.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 1 : Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour éviter l'erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cellule 2 : Configuration\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# On suppose que ton .env contient :\n",
    "# OPENAI_API_KEY=sk-xxxxxx\n",
    "# (ou autre variable si tu utilises Azure)\n",
    "#\n",
    "# Récupère la clé d'API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Clé API introuvable. Vérifie ton fichier .env.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client OpenAI initialisé avec succès !\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Client OpenAI\n",
    "# ============================\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Pour l'exemple, on définit le modèle par défaut\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "# Instanciation du client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    # Tu peux configurer d'autres options si besoin\n",
    ")\n",
    "\n",
    "print(\"Client OpenAI initialisé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rappels sur le Prompt Engineering avancé\n",
    "\n",
    "Plusieurs techniques de `prompt engineering` permettent d'améliorer les réponses d'un LLM:\n",
    "\n",
    "1. **Zero-shot prompting**  \n",
    "   On formule une simple question (ou instruction), sans donner d'exemples, et le modèle déduit la tâche.\n",
    "\n",
    "2. **Few-shot prompting**  \n",
    "   On fournit quelques exemples (input → output) pour guider le modèle vers le style, le format ou la logique attendus.\n",
    "\n",
    "3. **Chain-of-thought (CoT)**  \n",
    "   On incite le modèle à décomposer sa réflexion par étapes. Cela améliore souvent la justesse sur des problèmes complexes (calculs, raisonnement).\n",
    "\n",
    "4. **Self-refine**  \n",
    "   On demande au modèle de s'auto-corriger (self-critique) puis de proposer une version améliorée de sa réponse.\n",
    "\n",
    "Nous allons montrer quelques brefs exemples !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Prompt ===\n",
      "Prompt: Donne-moi 3 idées de recettes végétariennes à base de tomates.\n",
      "\n",
      "Réponse du modèle :\n",
      "\n",
      "Voici trois délicieuses idées de recettes végétariennes à base de tomates :\n",
      "\n",
      "### 1. **Tarte à la tomate et au basilic**\n",
      "**Ingrédients :**\n",
      "- Pâte brisée\n",
      "- Tomates mûres (variétés différentes pour plus de saveur)\n",
      "- Fromage de chèvre ou mozzarella\n",
      "- Basilic frais\n",
      "- Huile d'olive\n",
      "- Sel et poivre\n",
      "\n",
      "**Instructions :**\n",
      "1. Préchauffez le four à 180°C.\n",
      "2. Étalez la pâte brisée dans un moule à tarte et piquez le fond avec une fourchette.\n",
      "3. Coupez les tomates en rondelles et disposez-les sur la pâte.\n",
      "4. Émiettez le fromage de chèvre ou coupez la mozzarella en tranches et ajoutez-les sur les tomates.\n",
      "5. Arrosez d'un filet d'huile d'olive, salez et poivrez.\n",
      "6. Faites cuire au four pendant \n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Zero-shot\n",
    "# ============================\n",
    "\n",
    "prompt_1 = \"Donne-moi 3 idées de recettes végétariennes à base de tomates.\"\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_1}\n",
    "    ],\n",
    "    # Contrôle du style\n",
    "    max_tokens=200,\n",
    "    temperature=0.7  # plus la température est haute, plus c'est créatif\n",
    ")\n",
    "\n",
    "print(\"=== Zero-shot Prompt ===\")\n",
    "print(f\"Prompt: {prompt_1}\\n\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, pas d’exemples ni d’instructions détaillées, on se contente d’un prompt direct.\n",
    "\n",
    "\n",
    "\n",
    "###  Exemple Few-shot prompting (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Few-shot Prompt ===\n",
      "Réponse du modèle :\n",
      "\n",
      "- Ratatouille aux légumes de saison et tomates fraîches\n",
      "- Stuffed bell peppers (poivrons farcis) avec un mélange de riz, tomates, haricots et épices\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 6 : Few-shot\n",
    "# ============================\n",
    "\n",
    "few_shot_prompt = \"\"\"\n",
    "Tu es un assistant culinaire spécialisé en recettes végétariennes.\n",
    "Voici des exemples :\n",
    "\n",
    "Exemple 1:\n",
    "Q: Quelles idées de salade d'été me proposes-tu ?\n",
    "A: - Salade de quinoa et tomates cerises\n",
    "   - Salade de lentilles aux oignons rouges\n",
    "   - ...\n",
    "\n",
    "Exemple 2:\n",
    "Q: Je veux cuisiner des champignons, as-tu une idée ?\n",
    "A: - Poêlée de champignons, ail et persil\n",
    "   - ...\n",
    "\n",
    "Maintenant, voici ma question:\n",
    "\n",
    "Q: Propose-moi 2 plats végétariens sans gluten, si possible avec des tomates.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt}\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=== Few-shot Prompt ===\")\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous donnons au modèle **deux exemples** de questions/réponses avant la **véritable question**. Cela oriente le style et le contexte.\n",
    "\n",
    "\n",
    "\n",
    "###  7 : Exemple Chain-of-thought (Code)\n",
    "\n",
    "On va demander un **calcul** simple, en guidant le modèle à réfléchir pas à pas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain-of-thought Prompt ===\n",
      "Réponse du modèle (avec raisonnement) :\n",
      "\n",
      "Pour résoudre ce problème, nous allons suivre les étapes une par une.\n",
      "\n",
      "1. **Alice commence avec 5 pommes.**\n",
      "   - Pommes d'Alice : 5\n",
      "\n",
      "2. **Elle en jette 2.**\n",
      "   - Pommes d'Alice après avoir jeté 2 pommes : 5 - 2 = 3\n",
      "   - Pommes d'Alice : 3\n",
      "\n",
      "3. **Elle en donne 1 à Bob.**\n",
      "   - Pommes d'Alice après avoir donné 1 pomme à Bob : 3 - 1 = 2\n",
      "   - Pommes d'Alice : 2\n",
      "\n",
      "4. **Bob lui rend ensuite 1 pomme.**\n",
      "   - Pommes d'Alice après que Bob lui a rendu 1 pomme : 2 + 1 = 3\n",
      "   - Pommes d'Alice : 3\n",
      "\n",
      "En résumé, après toutes ces actions, Alice a 3 pommes à la fin.\n",
      "\n",
      "**Réponse finale\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Chain-of-thought\n",
    "# ============================\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "Alice a 5 pommes, elle en jette 2, puis elle en donne 1 à Bob.\n",
    "Bob lui rend ensuite 1 pomme.\n",
    "Combien de pommes Alice a-t-elle à la fin ?\n",
    "\n",
    "Explique ton raisonnement étape par étape, puis donne la réponse finale.\n",
    "\"\"\"\n",
    "\n",
    "response_3 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.2  # on réduit la température pour moins de fantaisie\n",
    ")\n",
    "\n",
    "print(\"=== Chain-of-thought Prompt ===\")\n",
    "print(\"Réponse du modèle (avec raisonnement) :\\n\")\n",
    "print(response_3.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On demande explicitement « explique ton raisonnement ». Cela **n’oblige** pas le modèle à le faire, mais en pratique, GPT-4o-mini (ou tout modèle qui gère le CoT) fournit souvent une solution pas-à-pas.\n",
    "\n",
    "---\n",
    "\n",
    "###  8 : Exemple Self-refine (Code)\n",
    "\n",
    "L’idée : on fait **une première demande** (première réponse) et ensuite **on redemande** au modèle de s’auto-corriger.\n",
    "\n",
    "#### 8a. Premier prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (1) : Code buggy ===\n",
      "\n",
      "Voici une fonction Python qui calcule la somme d'une liste, avec un bug volontaire inclus :\n",
      "\n",
      "```python\n",
      "def somme_liste(nums):\n",
      "    total = 0\n",
      "    for num in nums:\n",
      "        total += num + 1  # Bug : on ajoute 1 à chaque élément\n",
      "    return total\n",
      "\n",
      "# Exemples d'utilisation\n",
      "ma_liste = [1, 2, 3, 4]\n",
      "resultat = somme_liste(ma_liste)\n",
      "print(\"La somme de la liste est :\", resultat)\n",
      "```\n",
      "\n",
      "Dans cette fonction, un bug a été introduit en ajoutant 1 à chaque élément de la liste lors du calcul de la somme. Cela fausse le résultat final.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8a : Self-refine Step 1\n",
    "# ============================\n",
    "\n",
    "prompt_sr1 = \"\"\"\n",
    "Ecris une courte fonction Python pour calculer la somme d'une liste. \n",
    "Ajoute un bug volontaire dans le code. \n",
    "\"\"\"\n",
    "\n",
    "response_sr1 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr1}],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "buggy_code = response_sr1.choices[0].message.content\n",
    "\n",
    "print(\"=== Self-refine (1) : Code buggy ===\\n\")\n",
    "print(buggy_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8b. Self-critique et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-refine (2) : Correction ===\n",
      "\n",
      "Bien sûr ! Analysons le code que vous avez fourni.\n",
      "\n",
      "### Analyse du code\n",
      "\n",
      "La fonction `somme_liste` est censée calculer la somme des éléments d'une liste de nombres. Cependant, il y a un bug dans la ligne suivante :\n",
      "\n",
      "```python\n",
      "total += num + 1  # Bug : on ajoute 1 à chaque élément\n",
      "```\n",
      "\n",
      "Ici, au lieu d'ajouter simplement `num` à `total`, on ajoute `num + 1`, ce qui signifie que chaque élément de la liste contribue à la somme avec une valeur augmentée de 1. Cela fausse le résultat final.\n",
      "\n",
      "### Correctif\n",
      "\n",
      "Pour corriger ce bug, il suffit de modifier la ligne incriminée pour qu'elle ajoute uniquement `num` à `total` :\n",
      "\n",
      "```python\n",
      "total += num  # Correction : on ajoute uniquement l'élément\n",
      "```\n",
      "\n",
      "### Version améliorée du code\n",
      "\n",
      "Nous pouvons également améliorer la fonction en utilisant la fonction intégrée `sum()` de Python, qui est plus concise et efficace pour calculer la somme d'une liste. Voici la version corrigée et améliorée :\n",
      "\n",
      "```python\n",
      "def somme_liste(nums):\n",
      "    return sum(nums)  # Utilisation de la fonction intégrée sum\n",
      "\n",
      "# Exemples d'utilisation\n",
      "ma_liste = [1, 2, 3, 4]\n",
      "resultat = somme_liste(ma_liste)\n",
      "print(\"La somme de la liste est :\", resultat)\n",
      "```\n",
      "\n",
      "### Explication de la correction\n",
      "\n",
      "1. **Correction du bug** : En changeant `total += num + 1` à `total += num`, nous nous assurons que nous ajoutons uniquement la valeur de l'élément actuel à la somme, ce qui donne le résultat attendu.\n",
      "\n",
      "2. **Amélioration de la lisibilité et de l'efficacité** : En utilisant la fonction intégrée `sum()`, nous simplifions le code, le rendant plus lis\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 8b : Self-refine Step 2\n",
    "# ============================\n",
    "\n",
    "prompt_sr2 = f\"\"\"\n",
    "Voici un code Python qui contient un bug:\n",
    "\n",
    "{buggy_code}\n",
    "\n",
    "Peux-tu l'analyser, détecter le bug, proposer un correctif et une version améliorée du code ? \n",
    "Explique la correction.\n",
    "\"\"\"\n",
    "\n",
    "response_sr2 = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_sr2}],\n",
    "    max_tokens=400,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"=== Self-refine (2) : Correction ===\\n\")\n",
    "print(response_sr2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on utilise la première réponse pour nourrir le second prompt, demandant au modèle de **critiquer** et **améliorer** la réponse initiale.\n",
    "\n",
    "---\n",
    "\n",
    "### Cellule 9 : Interactive Prompt (Code)\n",
    "\n",
    "Enfin, on peut proposer une **cellule interactive** : l’utilisateur peut saisir un prompt, et on envoie la requête au modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Réponse du modèle ===\n",
      "Salut ! Comment puis-je t'aider aujourd'hui ?\n",
      "---------------------------------------------------\n",
      "\n",
      "\n",
      "=== Réponse du modèle ===\n",
      "Je suis désolé, mais je ne peux pas fournir de prévisions météorologiques en temps réel. Je vous recommande de consulter un site de météo ou une application pour obtenir les informations les plus récentes sur la météo à Paris cette semaine.\n",
      "---------------------------------------------------\n",
      "\n",
      "Fin de l'interaction.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 9 : Prompt interactif\n",
    "# ============================\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Tape ton prompt ('exit' pour quitter) : \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Fin de l'interaction.\")\n",
    "        break\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\":\"user\",\"content\":user_input}],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Réponse du modèle ===\")\n",
    "    print(resp.choices[0].message.content)\n",
    "    print(\"---------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, tu peux saisir n’importe quel prompt, et tu verras la réponse du modèle.  \n",
    "Tape `exit` pour quitter la boucle.\n",
    "\n",
    "----\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Dans ce Notebook, nous avons :\n",
    "\n",
    "- configuré la nouvelle API `openai >= 1.0.0` (avec `client.chat.completions.create(...)`),\n",
    "- utilisé plusieurs techniques de **prompt engineering avancé** : \n",
    "  - zero-shot, \n",
    "  - few-shot, \n",
    "  - chain-of-thought,\n",
    "  - self-refine,\n",
    "  - un prompt interactif final."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
